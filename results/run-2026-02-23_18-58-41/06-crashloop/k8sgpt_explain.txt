   0% |                                                | (0/3, 0 it/hr) [0s:0s]                                                                                 33% |██████████████                               | (1/3, 3 it/min) [20s:41s]                                                                                 66% |█████████████████████████████                | (2/3, 6 it/min) [41s:10s]                                                                                100% |█████████████████████████████████████████████| (3/3, 3 it/min)
AI Provider: ollama

0: Deployment 06-crashloop/crash-app()
- Error: Deployment 06-crashloop/crash-app has 1 replicas but 0 are available with status running
Error: Deployment "06-crashloop/crash-app" has 1 replicas but 0 are available with status running.

Solution: 
1. Check logs for crash-looping pods using `kubectl get pods -n <namespace> | grep crash`
2. Identify the root cause of the crash and fix the issue
3. Roll out a new deployment with the fixed image or code changes
4. Verify the new deployment is available and running with `kubectl get pods -n <namespace>`
1: ConfigMap 06-crashloop/kube-root-ca.crt()
- Error: ConfigMap kube-root-ca.crt is not used by any pods in the namespace
Error: ConfigMap kube-root-ca.crt is not used by any pods in the namespace.

Solution: 
1. Check if the ConfigMap is created correctly using `kubectl get configmap kube-root-ca.crt -o yaml`.
2. Verify if the pods are using the correct ConfigMap name and namespace.
3. If no pods are using it, check for any deprecated or removed references to the ConfigMap in your deployment YAML files.
4. Update or recreate the pods that should be using this ConfigMap.
2: Pod 06-crashloop/crash-app-85c68bcc8d-jkq6g(Deployment/crash-app)
- Error: the last termination reason is Error container=crash pod=crash-app-85c68bcc8d-jkq6g
Error: The pod "crash-app-85c68bcc8d-jkq6g" terminated due to a container crash.

Solution: 
1. Check the container logs for errors using `kubectl logs -n <namespace> crash-app-85c68bcc8d-jkq6g`.
2. Verify the container image and configuration.
3. If the issue persists, try restarting the pod with `kubectl rollout restart deployment <deployment-name>`.

